{
    "9f672d69432e6339fc69c6848b384bd6bb744051": {
        "authored_data": "2014 Apr 21 20:10",
        "commit.message": "use mid point of a row group to decide to create a split or not\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "70707e4bd2f5b41a08d4b5a306d5876473532e01",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        160,
                        180,
                        181,
                        185,
                        187,
                        188,
                        189,
                        199,
                        206,
                        214,
                        215,
                        216,
                        217,
                        218
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        289,
                        290,
                        291
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        160,
                        161,
                        179,
                        180,
                        183,
                        184,
                        188,
                        189,
                        190,
                        191,
                        192,
                        194,
                        195,
                        196,
                        197,
                        198,
                        199,
                        200,
                        201,
                        202,
                        203,
                        204,
                        205,
                        206,
                        207,
                        208,
                        218,
                        225,
                        233,
                        234,
                        235,
                        236,
                        237
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        308,
                        309,
                        310
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/metadata/BlockMetaData.java": {
                "new": {
                    "('parquet.hadoop.metadata#BlockMetaData', None)": [
                        110,
                        111,
                        112
                    ],
                    "('parquet.hadoop.metadata#BlockMetaData', 'getCompressedSize')": [
                        113,
                        114,
                        115,
                        116,
                        117,
                        118
                    ],
                    "(None, None)": [
                        119
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/TestInputFormat.java": {
                "old": {
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsAlignedWithHDFSBlock')": [
                        100,
                        105
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testRowGroupNotAlignToHDFSBlock')": [
                        111,
                        117,
                        124
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsNotAlignedWithHDFSBlock')": [
                        137,
                        143,
                        149
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsSmallerThanMaxSizeAndAlignToHDFS')": [
                        164,
                        165,
                        166,
                        167,
                        168,
                        169,
                        170,
                        174,
                        175,
                        176,
                        185
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsCrossHDFSBlockBoundaryToSatisfyMinSize')": [
                        198
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testMultipleRowGroupsInABlockToAlignHDFSBlock')": [
                        211,
                        212,
                        213,
                        214,
                        215,
                        216,
                        220,
                        221,
                        222,
                        232
                    ],
                    "('parquet.hadoop#TestInputFormat', 'newBlock(long,long)')": [
                        275,
                        279,
                        281
                    ]
                },
                "new": {
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsAlignedWithHDFSBlock')": [
                        100,
                        105
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testRowGroupNotAlignToHDFSBlock')": [
                        116,
                        123,
                        124,
                        125,
                        126,
                        127,
                        128,
                        129,
                        130,
                        131,
                        132,
                        133,
                        134,
                        135
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsNotAlignedWithHDFSBlock')": [
                        148,
                        154,
                        160
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsSmallerThanMaxSizeAndAlignToHDFS')": [
                        175,
                        176,
                        177,
                        178,
                        182,
                        183,
                        184,
                        193
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsCrossHDFSBlockBoundaryToSatisfyMinSize')": [
                        206
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testMultipleRowGroupsInABlockToAlignHDFSBlock')": [
                        219,
                        220,
                        221,
                        222,
                        223,
                        224,
                        225,
                        229,
                        230,
                        231,
                        241
                    ],
                    "('parquet.hadoop#TestInputFormat', 'newBlock(long,long)')": [
                        284,
                        286,
                        289,
                        291
                    ]
                }
            }
        }
    },
    "05c3e2706a356ea878177faf93ed108a348bc7ae": {
        "authored_data": "2014 Apr 21 11:57",
        "commit.message": "ensure SimpleRecord#getValues() is unmodifiable\n\nThis avoids modification from the outside",
        "commit.author.name": "Miguel Ping",
        "pcid": "f5c3151d057708a7377430b6c51621071656d10e",
        "changes": {
            "parquet-tools/src/main/java/parquet/tools/read/SimpleRecord.java": {
                "old": {
                    "('parquet.tools.read#SimpleRecord', 'getValues')": [
                        38
                    ]
                },
                "new": {
                    "(None, None)": [
                        21
                    ],
                    "('parquet.tools.read#SimpleRecord', 'getValues')": [
                        39
                    ]
                }
            }
        }
    },
    "70707e4bd2f5b41a08d4b5a306d5876473532e01": {
        "authored_data": "2014 Apr 18 21:20",
        "commit.message": "use getStartingPos for BlockMetadata, which returns the startingPos for the first Column\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "9705f4905a5077ec7208a0fa3e230668157fe471",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        185,
                        189,
                        193,
                        194,
                        195,
                        196,
                        197
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        185,
                        189
                    ]
                }
            }
        }
    },
    "9705f4905a5077ec7208a0fa3e230668157fe471": {
        "authored_data": "2014 Apr 18 21:16",
        "commit.message": "1. check row groups are sorted; 2. add getStartingPos for BlockMetadata, which returns the startingPos for the first Column\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "00d631c15d147e7e7a07d65fe50999c878a921a8",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        291
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        291,
                        292
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'checkSorted(List)')": [
                        317,
                        318,
                        319,
                        320,
                        321,
                        322
                    ],
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        323,
                        324,
                        325
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/metadata/BlockMetaData.java": {
                "new": {
                    "('parquet.hadoop.metadata#BlockMetaData', None)": [
                        98,
                        99,
                        100,
                        101,
                        104
                    ],
                    "('parquet.hadoop.metadata#BlockMetaData', 'getStartingPos')": [
                        102,
                        103
                    ]
                }
            }
        }
    },
    "00d631c15d147e7e7a07d65fe50999c878a921a8": {
        "authored_data": "2014 Apr 18 21:08",
        "commit.message": "make SplitInfo contain the hdfsBlock\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "8e348e60b822d86a7e0862902c315beb56388725",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        206,
                        209,
                        210,
                        226,
                        227,
                        228,
                        229,
                        234,
                        235,
                        264
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        276,
                        287,
                        288,
                        289,
                        290,
                        291,
                        292,
                        294,
                        295,
                        296,
                        303,
                        315
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        202,
                        203,
                        204,
                        205,
                        210,
                        213,
                        214,
                        234,
                        263
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        275,
                        286,
                        287,
                        288,
                        289,
                        291,
                        298,
                        310
                    ]
                }
            }
        }
    },
    "8e348e60b822d86a7e0862902c315beb56388725": {
        "authored_data": "2014 Apr 18 20:51",
        "commit.message": "create a getStartingPos in ColumnChunkMetaData\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "a85b7fddbbdd381378a7075741be74c19c681ed0",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetFileReader.java": {
                "old": {
                    "('parquet.hadoop#ParquetFileReader', 'readNextRowGroup')": [
                        349
                    ],
                    "('parquet.hadoop#ParquetFileReader', None)": [
                        369,
                        370,
                        371,
                        372,
                        380
                    ],
                    "('parquet.hadoop#ParquetFileReader', 'getStartingPos(ColumnChunkMetaData)')": [
                        373,
                        374,
                        375,
                        376,
                        377,
                        378,
                        379
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetFileReader', 'readNextRowGroup')": [
                        349
                    ],
                    "('parquet.hadoop#ParquetFileReader', None)": [
                        369
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        195,
                        196,
                        197,
                        198,
                        199,
                        200,
                        209
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        294,
                        299,
                        306
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        195,
                        196,
                        197,
                        206
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        291,
                        296,
                        303
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/metadata/ColumnChunkMetaData.java": {
                "old": {
                    "('parquet.hadoop.metadata#LongColumnChunkMetaData', None)": [
                        227,
                        238,
                        246,
                        252
                    ],
                    "('parquet.hadoop.metadata#LongColumnChunkMetaData', 'getFirstDataPageOffset')": [
                        263
                    ]
                },
                "new": {
                    "('parquet.hadoop.metadata#ColumnChunkMetaData', None)": [
                        61,
                        62,
                        71,
                        72,
                        73
                    ],
                    "('parquet.hadoop.metadata#ColumnChunkMetaData', 'getStartingPos')": [
                        63,
                        64,
                        65,
                        66,
                        67,
                        68,
                        69,
                        70
                    ],
                    "('parquet.hadoop.metadata#LongColumnChunkMetaData', None)": [
                        240,
                        251,
                        259,
                        265
                    ],
                    "('parquet.hadoop.metadata#LongColumnChunkMetaData', 'getFirstDataPageOffset')": [
                        276
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/TestInputFormat.java": {
                "old": {
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsSmallerThanMinSplitSize')": [
                        75
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsNegative')": [
                        86
                    ]
                },
                "new": {
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsSmallerThanMinSplitSize')": [
                        75
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsNegative')": [
                        86
                    ]
                }
            }
        }
    },
    "a85b7fddbbdd381378a7075741be74c19c681ed0": {
        "authored_data": "2014 Apr 18 17:40",
        "commit.message": "better message\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "83e34bec54298265c50e366fae364c02a9e2dfe3",
        "changes": {
            "parquet-hadoop/src/test/java/parquet/hadoop/TestInputFormat.java": {
                "old": {
                    "('parquet.hadoop#TestInputFormat', None)": [
                        53
                    ]
                },
                "new": {
                    "('parquet.hadoop#TestInputFormat', None)": [
                        53
                    ]
                }
            }
        }
    },
    "83e34bec54298265c50e366fae364c02a9e2dfe3": {
        "authored_data": "2014 Apr 18 16:53",
        "commit.message": "add non-negative check in generateSplits method\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "ac816d91e2c30f7bceacb8601ae13a0ab0107277",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        285,
                        286
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', 'generateSplits(List,BlockLocation,FileStatus,FileMetaData,String,Map,long,long)')": [
                        285,
                        286
                    ]
                }
            }
        }
    },
    "ac816d91e2c30f7bceacb8601ae13a0ab0107277": {
        "authored_data": "2014 Apr 18 16:49",
        "commit.message": "min split size default to 0\n",
        "commit.author.name": "Tianshuo Deng",
        "pcid": "9814332cc3fed776eab8ebd03bbbc241ec562c15",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(Configuration,List)')": [
                        344
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(Configuration,List)')": [
                        344
                    ]
                }
            }
        }
    }
}