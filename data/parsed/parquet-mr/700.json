{
    "ce65dfb394623c34dd7919aba5c0687f1bcf39f2": {
        "authored_data": "2015 Feb 05 23:06",
        "commit.message": "PARQUET-139: Avoid reading footers when using task-side metadata\n\nThis updates the InternalParquetRecordReader to initialize the ReadContext in each task rather than once for an entire job. There are two reasons for this change:\n\n1. For correctness, the requested projection schema must be validated against each file schema, not once using the merged schema.\n2. To avoid reading file footers on the client side, which is a performance bottleneck.\n\nBecause the read context is reinitialized in every task, it is no longer necessary to pass the its contents to each task in ParquetInputSplit. The fields and accessors have been removed.\n\nThis also adds a new InputFormat, ParquetFileInputFormat that uses FileSplits instead of ParquetSplits. It goes through the normal ParquetRecordReader and creates a ParquetSplit on the task side. This is to avoid accidental behavior changes in ParquetInputFormat.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #91 from rdblue/PARQUET-139-input-format-task-side and squashes the following commits:\n\ncb30660 [Ryan Blue] PARQUET-139: Fix deprecated reader bug from review fixes.\n09cde8d [Ryan Blue] PARQUET-139: Implement changes from reviews.\n3eec553 [Ryan Blue] PARQUET-139: Merge new InputFormat into ParquetInputFormat.\n8971b80 [Ryan Blue] PARQUET-139: Add ParquetFileInputFormat that uses FileSplit.\n87dfe86 [Ryan Blue] PARQUET-139: Expose read support helper methods.\n057c7dc [Ryan Blue] PARQUET-139: Update reader to initialize read context in tasks.\n",
        "commit.author.name": "Ryan Blue",
        "pcid": "05adc21b15dbe30d9bded0cde56f482f1c932d6f",
        "changes": {
            "parquet-avro/src/main/java/parquet/avro/AvroReadSupport.java": {
                "old": {
                    "('parquet.avro#AvroReadSupport', 'init(Configuration,Map,MessageType)')": [
                        80
                    ]
                },
                "new": {
                    "('parquet.avro#AvroReadSupport', 'init(Configuration,Map,MessageType)')": [
                        80,
                        81,
                        82
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/InternalParquetRecordReader.java": {
                "old": {
                    "('parquet.hadoop#InternalParquetRecordReader', 'initialize(MessageType,MessageType,Map,Map,Path,List,Configuration)')": [
                        158,
                        159,
                        162,
                        165,
                        167,
                        168
                    ]
                },
                "new": {
                    "(None, None)": [
                        22,
                        23,
                        24,
                        28,
                        38,
                        242,
                        243
                    ],
                    "('parquet.hadoop#InternalParquetRecordReader', 'initialize(MessageType,Map,Path,List,Configuration)')": [
                        163,
                        164,
                        167,
                        168,
                        169,
                        170,
                        173,
                        175
                    ],
                    "('parquet.hadoop#InternalParquetRecordReader', None)": [
                        233
                    ],
                    "('parquet.hadoop#InternalParquetRecordReader', 'toSetMultiMap(Map)')": [
                        234,
                        235,
                        236,
                        237,
                        238,
                        239,
                        240,
                        241
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        207,
                        213,
                        214,
                        215,
                        216,
                        217,
                        238,
                        267
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getReadSupport(Configuration)')": [
                        239,
                        241,
                        242,
                        243,
                        244,
                        245
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(JobContext)')": [
                        259
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(Configuration,List)')": [
                        269,
                        282
                    ],
                    "('parquet.hadoop#SplitStrategy', None)": [
                        492,
                        493,
                        494,
                        502,
                        503,
                        504
                    ],
                    "('parquet.hadoop#SplitStrategy', 'getSplitStrategy(boolean)')": [
                        495,
                        496,
                        497,
                        498,
                        499,
                        500,
                        501
                    ],
                    "('parquet.hadoop#SplitStrategy', 'getSplits(Configuration,List,long,long,ReadContext)')": [
                        505,
                        506,
                        507,
                        508,
                        509
                    ],
                    "(None, None)": [
                        510,
                        601,
                        602
                    ],
                    "('parquet.hadoop#TaskSideMetadataSplitStrategy', None)": [
                        511,
                        512,
                        513,
                        533,
                        534,
                        544,
                        545
                    ],
                    "('parquet.hadoop#TaskSideMetadataSplitStrategy', 'getSplits(Configuration,List,long,long,ReadContext)')": [
                        514,
                        515,
                        516,
                        517,
                        518,
                        519,
                        520,
                        521,
                        522,
                        523,
                        524,
                        525,
                        526,
                        527,
                        528,
                        529,
                        530,
                        531,
                        532
                    ],
                    "('parquet.hadoop#TaskSideMetadataSplitStrategy', 'findBlockIndex(BlockLocation,long)')": [
                        535,
                        536,
                        537,
                        538,
                        539,
                        540,
                        541,
                        542,
                        543
                    ],
                    "('parquet.hadoop#TaskSideMetadataSplitStrategy', 'generateTaskSideMDSplits(BlockLocation,FileStatus,String,Map,long,long)')": [
                        546,
                        547,
                        548,
                        549,
                        550,
                        551,
                        552,
                        553,
                        554,
                        555,
                        556,
                        557,
                        558,
                        559,
                        560,
                        561,
                        562,
                        563,
                        564,
                        565,
                        566,
                        567,
                        568,
                        569,
                        570,
                        571,
                        572,
                        573,
                        574,
                        575,
                        576,
                        577,
                        578,
                        579,
                        580,
                        581,
                        582,
                        583,
                        584,
                        585,
                        586,
                        587,
                        588,
                        589,
                        590,
                        591,
                        592,
                        593,
                        594,
                        595,
                        596,
                        597,
                        598,
                        599,
                        600
                    ],
                    "('parquet.hadoop#ClientSideMetadataSplitStrategy', None)": [
                        603,
                        715,
                        716,
                        717,
                        724
                    ]
                },
                "new": {
                    "(None, None)": [
                        51,
                        53,
                        539
                    ],
                    "('parquet.hadoop#ParquetInputFormat', None)": [
                        209,
                        215,
                        216,
                        217,
                        218,
                        219,
                        220,
                        221,
                        222,
                        223,
                        224,
                        225,
                        246,
                        247,
                        248,
                        249,
                        254,
                        255,
                        256,
                        257,
                        258,
                        259,
                        260,
                        264,
                        265,
                        266,
                        267,
                        268,
                        269,
                        270,
                        312,
                        313,
                        314
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getReadSupport(Configuration)')": [
                        250,
                        251,
                        252,
                        253
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getReadSupportInstance(Configuration)')": [
                        261,
                        262,
                        263
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getReadSupportInstance(Class)')": [
                        271,
                        272,
                        274
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(JobContext)')": [
                        288,
                        289,
                        290,
                        291,
                        292,
                        293,
                        294,
                        295,
                        296,
                        297,
                        298,
                        299,
                        300,
                        301,
                        302,
                        303,
                        304
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(Configuration,List)')": [
                        328,
                        329
                    ],
                    "('parquet.hadoop#ClientSideMetadataSplitStrategy', None)": [
                        540,
                        652
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputSplit.java": {
                "old": {
                    "(None, None)": [
                        29,
                        32,
                        41,
                        281,
                        282,
                        283,
                        284
                    ],
                    "('parquet.hadoop#ParquetInputSplit', None)": [
                        62,
                        63,
                        74,
                        96,
                        97,
                        98,
                        99,
                        100,
                        133,
                        134,
                        138,
                        139,
                        140,
                        144,
                        145,
                        146,
                        147,
                        148,
                        149,
                        150,
                        160,
                        161,
                        162,
                        163,
                        164,
                        238,
                        239,
                        243,
                        244,
                        271,
                        272
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'getRequestedSchema')": [
                        151,
                        152
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'getReadSupportMetadata')": [
                        165,
                        166
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'toString')": [
                        193,
                        194
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readFields(DataInput)')": [
                        213,
                        214
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'write(DataOutput)')": [
                        234,
                        235
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeUTF8(DataOutput,String)')": [
                        240,
                        241,
                        242
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readUTF8(DataInput)')": [
                        245,
                        246,
                        247
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readKeyValues(DataInput)')": [
                        262,
                        263,
                        264,
                        265,
                        266,
                        267,
                        268,
                        269,
                        270
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeKeyValues(DataOutput,Map)')": [
                        273,
                        274,
                        275,
                        276,
                        277,
                        278,
                        279,
                        280
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputSplit', None)": [
                        69,
                        91,
                        118,
                        119,
                        120,
                        121,
                        122,
                        123,
                        128,
                        129,
                        130,
                        131,
                        132,
                        133,
                        134,
                        135,
                        136,
                        137,
                        142,
                        143,
                        144,
                        154
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'from(FileSplit)')": [
                        124,
                        125,
                        126,
                        127
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'from(org)')": [
                        138,
                        139,
                        140,
                        141
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetReader.java": {
                "old": {
                    "(None, None)": [
                        28,
                        29,
                        41,
                        43,
                        45
                    ],
                    "('parquet.hadoop#ParquetReader', None)": [
                        56,
                        58,
                        128,
                        129,
                        130,
                        131
                    ],
                    "('parquet.hadoop#ParquetReader', 'initReader')": [
                        161,
                        164,
                        165,
                        166
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetReader', 'initReader')": [
                        150,
                        151,
                        152,
                        153,
                        156,
                        157,
                        158
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetRecordReader.java": {
                "old": {
                    "(None, None)": [
                        47,
                        54
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,TaskAttemptContext)')": [
                        141
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,Configuration,Reporter)')": [
                        147
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initializeInternalReader(ParquetInputSplit,Configuration)')": [
                        192,
                        194,
                        196,
                        197,
                        198,
                        199
                    ]
                },
                "new": {
                    "(None, None)": [
                        43,
                        215,
                        216
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,TaskAttemptContext)')": [
                        140
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,Configuration,Reporter)')": [
                        146
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initializeInternalReader(ParquetInputSplit,Configuration)')": [
                        193
                    ],
                    "('parquet.hadoop#ParquetRecordReader', None)": [
                        203
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'toParquetSplit(InputSplit)')": [
                        204,
                        205,
                        206,
                        207,
                        208,
                        209,
                        210,
                        211,
                        212,
                        213,
                        214
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/mapred/DeprecatedParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', 'getRecordReader(InputSplit,JobConf,Reporter)')": [
                        45
                    ],
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', None)": [
                        77,
                        78,
                        79,
                        80,
                        81,
                        85,
                        88
                    ]
                },
                "new": {
                    "(None, None)": [
                        21,
                        29
                    ],
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', 'getRecordReader(InputSplit,JobConf,Reporter)')": [
                        47
                    ],
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', 'getSplits(JobConf,int)')": [
                        52,
                        53,
                        54,
                        55
                    ],
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', None)": [
                        83,
                        84,
                        85,
                        89,
                        90,
                        93,
                        94,
                        95,
                        96,
                        97,
                        98,
                        99,
                        100,
                        174,
                        175
                    ],
                    "('parquet.hadoop.mapred#DeprecatedParquetInputFormat', 'isTaskSideMetaData(JobConf)')": [
                        172,
                        173
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/TestInputFormat.java": {
                "old": {
                    "(None, None)": [
                        38
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsSmallerThanMinSplitSizeTaskSide')": [
                        111,
                        112,
                        113,
                        114,
                        115,
                        116,
                        117
                    ],
                    "('parquet.hadoop#TestInputFormat', None)": [
                        118,
                        119,
                        120,
                        121,
                        129,
                        130,
                        131,
                        132,
                        177,
                        178,
                        179,
                        180,
                        224,
                        248,
                        249,
                        273,
                        274,
                        275,
                        276,
                        330,
                        355,
                        356,
                        368,
                        369,
                        370,
                        414,
                        443,
                        444,
                        574,
                        575
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testThrowExceptionWhenMaxSplitSizeIsNegative')": [
                        125,
                        126,
                        127,
                        128
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSThrowExceptionWhenMaxSplitSizeIsNegative')": [
                        133,
                        134,
                        135
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsAlignedWithHDFSBlock')": [
                        175,
                        176
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSGenerateSplitsAlignedWithHDFSBlock')": [
                        181,
                        182,
                        183,
                        184,
                        185,
                        186,
                        187,
                        188,
                        189
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSRowGroupNotAlignToHDFSBlock')": [
                        225,
                        226,
                        227,
                        228,
                        229,
                        230,
                        231,
                        232,
                        233,
                        234,
                        235,
                        236,
                        237,
                        238,
                        239,
                        240,
                        241,
                        242,
                        243,
                        244,
                        245,
                        246,
                        247
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSGenerateSplitsNotAlignedWithHDFSBlock')": [
                        277,
                        278,
                        279,
                        280,
                        281,
                        282,
                        283,
                        284,
                        285,
                        286,
                        287,
                        288,
                        289,
                        290,
                        291,
                        292,
                        293,
                        294
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSGenerateSplitsSmallerThanMaxSizeAndAlignToHDFS')": [
                        331,
                        332,
                        333,
                        334,
                        335,
                        336,
                        337,
                        338,
                        339,
                        340,
                        341,
                        342,
                        343,
                        344,
                        345,
                        346,
                        347,
                        348,
                        349,
                        350,
                        351,
                        352,
                        353,
                        354
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testGenerateSplitsCrossHDFSBlockBoundaryToSatisfyMinSize')": [
                        367
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSGenerateSplitsCrossHDFSBlockBoundaryToSatisfyMinSize')": [
                        371,
                        372,
                        373,
                        374,
                        375
                    ],
                    "('parquet.hadoop#TestInputFormat', 'testTSMultipleRowGroupsInABlockToAlignHDFSBlock')": [
                        415,
                        416,
                        417,
                        418,
                        419,
                        420,
                        421,
                        422,
                        423,
                        424,
                        425,
                        426,
                        427,
                        428,
                        429,
                        430,
                        431,
                        432,
                        433,
                        434,
                        435,
                        436,
                        437,
                        438,
                        439,
                        440,
                        441,
                        442
                    ],
                    "('parquet.hadoop#TestInputFormat', 'generateTSSplitByMinMaxSize(long,long)')": [
                        567,
                        568,
                        569,
                        570,
                        571,
                        572,
                        573
                    ],
                    "('parquet.hadoop#TestInputFormat', 'shouldSplitLocationBe(List,int)')": [
                        614
                    ]
                }
            },
            "parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java": {
                "old": {
                    "('org.apache.hadoop.hive.ql.io.parquet.read#ParquetRecordReaderWrapper', 'getSplit(InputSplit,JobConf)')": [
                        223,
                        224,
                        225
                    ]
                },
                "new": {
                    "('org.apache.hadoop.hive.ql.io.parquet.read#ParquetRecordReaderWrapper', 'getSplit(InputSplit,JobConf)')": [
                        223
                    ]
                }
            }
        }
    },
    "05adc21b15dbe30d9bded0cde56f482f1c932d6f": {
        "authored_data": "2015 Feb 05 22:36",
        "commit.message": "PARQUET-177: Added lower bound to memory manager resize\n\nPARQUET-177\n\nAuthor: Daniel Weeks <dweeks@netflix.com>\n\nCloses #115 from danielcweeks/memory-manager-limit and squashes the following commits:\n\nb2e4708 [Daniel Weeks] Updated to base memory allocation off estimated chunk size\n09d7aa3 [Daniel Weeks] Updated property name and default value\n8f6cff1 [Daniel Weeks] Added low bound to memory manager resize\n",
        "commit.author.name": "Daniel Weeks",
        "pcid": "668d031d7213d5e76cf39770ffce7f030c9bf056",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/InternalParquetRecordWriter.java": {
                "new": {
                    "('parquet.hadoop#InternalParquetRecordWriter', None)": [
                        183
                    ],
                    "('parquet.hadoop#InternalParquetRecordWriter', 'getSchema')": [
                        184,
                        185
                    ],
                    "(None, None)": [
                        186
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/MemoryManager.java": {
                "old": {
                    "('parquet.hadoop#MemoryManager', None)": [
                        48
                    ]
                },
                "new": {
                    "(None, None)": [
                        22
                    ],
                    "('parquet.hadoop#MemoryManager', None)": [
                        43,
                        47,
                        51,
                        55
                    ],
                    "('parquet.hadoop#MemoryManager', 'updateAllocation')": [
                        113,
                        114,
                        115,
                        116,
                        117,
                        120,
                        121,
                        122,
                        123,
                        124
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetOutputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(Configuration,Path,CompressionCodecName)')": [
                        294
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetOutputFormat', None)": [
                        111
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(Configuration,Path,CompressionCodecName)')": [
                        294,
                        295,
                        297
                    ]
                }
            }
        }
    },
    "668d031d7213d5e76cf39770ffce7f030c9bf056": {
        "authored_data": "2015 Feb 05 19:37",
        "commit.message": "PARQUET-181: Scrooge Write Support (take two)\n\nThis is similar to https://github.com/apache/incubator-parquet-mr/pull/43, but instead of making `ThriftWriteSupport` abstract, it keeps it around (but deprecated) and adds `AbstractThriftWriteSupport`. This is a little less elegant, but it seems to appease the semver overlords.\n\nAuthor: Colin Marc <colinmarc@gmail.com>\n\nCloses #58 from colinmarc/scrooge-write-support-2 and squashes the following commits:\n\ne2a0abd [Colin Marc] add write support to ParquetScroogeScheme\n19cf1a8 [Colin Marc] Add ScroogeWriteSupport and ParquetScroogeOutputFormat.\n",
        "commit.author.name": "Colin Marc",
        "pcid": "80417356f04c5ee1cd6f636e9b043db3f2de24f2",
        "changes": {
            "parquet-cascading/src/main/java/parquet/cascading/ParquetTBaseScheme.java": {
                "old": {
                    "(None, None)": [
                        33
                    ],
                    "('parquet.cascading#ParquetTBaseScheme', 'sinkConfInit(FlowProcess,Tap,JobConf)')": [
                        77,
                        78
                    ]
                },
                "new": {
                    "(None, None)": [
                        33
                    ],
                    "('parquet.cascading#ParquetTBaseScheme', 'sinkConfInit(FlowProcess,Tap,JobConf)')": [
                        77,
                        78
                    ]
                }
            },
            "parquet-scrooge/src/main/java/parquet/scrooge/ParquetScroogeScheme.java": {
                "old": {
                    "('parquet.scrooge#ParquetScroogeScheme', 'sinkConfInit(FlowProcess,Tap,JobConf)')": [
                        55,
                        56,
                        57
                    ],
                    "('parquet.scrooge#ParquetScroogeScheme', None)": [
                        59,
                        60,
                        61,
                        62,
                        63,
                        74,
                        75
                    ],
                    "('parquet.scrooge#ParquetScroogeScheme', 'isSink')": [
                        64
                    ],
                    "('parquet.scrooge#ParquetScroogeScheme', 'sink(FlowProcess,SinkCall)')": [
                        76,
                        77,
                        78
                    ],
                    "(None, None)": [
                        79
                    ]
                },
                "new": {
                    "(None, None)": [
                        35,
                        37
                    ],
                    "('parquet.scrooge#ParquetScroogeScheme', 'sinkConfInit(FlowProcess,Tap,JobConf)')": [
                        57,
                        58,
                        59,
                        60,
                        61
                    ]
                }
            },
            "parquet-scrooge/src/test/java/parquet/scrooge/ParquetScroogeSchemeTest.java": {
                "new": {
                    "(None, None)": [
                        35,
                        55,
                        56,
                        65,
                        235,
                        236
                    ],
                    "('parquet.scrooge#ParquetScroogeSchemeTest', None)": [
                        163,
                        164,
                        165,
                        166,
                        167,
                        168,
                        172,
                        173,
                        190,
                        191,
                        210,
                        211,
                        212,
                        213,
                        214,
                        215,
                        216,
                        217,
                        218,
                        219,
                        220,
                        221,
                        222,
                        223,
                        224,
                        225,
                        226,
                        227,
                        228,
                        229,
                        230,
                        231,
                        232,
                        233,
                        234
                    ],
                    "('parquet.scrooge#ParquetScroogeSchemeTest', 'testWriteThenRead')": [
                        169,
                        170,
                        171
                    ],
                    "('parquet.scrooge#ParquetScroogeSchemeTest', 'doWrite')": [
                        174,
                        175,
                        176,
                        177,
                        178,
                        179,
                        180,
                        181,
                        182,
                        183,
                        184,
                        185,
                        186,
                        187,
                        188,
                        189
                    ],
                    "('parquet.scrooge#ParquetScroogeSchemeTest', 'doRead')": [
                        192,
                        193,
                        194,
                        195,
                        196,
                        197,
                        198,
                        199,
                        200,
                        201,
                        202,
                        203,
                        204,
                        205,
                        206,
                        207,
                        208,
                        209
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/hadoop/thrift/ParquetThriftBytesOutputFormat.java": {
                "old": {
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setThriftClass(Job,Class)')": [
                        41
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'getThriftClass(Job)')": [
                        45
                    ]
                },
                "new": {
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setThriftClass(Job,Class)')": [
                        41
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'getThriftClass(Job)')": [
                        45
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/hadoop/thrift/ThriftBytesWriteSupport.java": {
                "old": {
                    "('parquet.hadoop.thrift#ThriftBytesWriteSupport', None)": [
                        70,
                        73
                    ],
                    "('parquet.hadoop.thrift#ThriftBytesWriteSupport', 'init(Configuration)')": [
                        107,
                        109,
                        113
                    ]
                },
                "new": {
                    "('parquet.hadoop.thrift#ThriftBytesWriteSupport', None)": [
                        70,
                        73
                    ],
                    "('parquet.hadoop.thrift#ThriftBytesWriteSupport', 'init(Configuration)')": [
                        107,
                        109,
                        113
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/hadoop/thrift/ThriftWriteSupport.java": {
                "old": {
                    "(None, None)": [
                        27,
                        29,
                        30,
                        31,
                        32,
                        33,
                        34,
                        35,
                        36,
                        37,
                        38,
                        39,
                        40,
                        41,
                        42,
                        43,
                        128,
                        130,
                        131
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', None)": [
                        45,
                        46,
                        63,
                        66,
                        67,
                        68,
                        69,
                        83,
                        84,
                        85,
                        96,
                        97,
                        105
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'setThriftClass(Configuration,Class)')": [
                        49
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'getThriftClass(Configuration)')": [
                        53,
                        54,
                        55,
                        56,
                        57,
                        58,
                        59,
                        60,
                        61,
                        62
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'init(Class)')": [
                        86,
                        87,
                        88,
                        89,
                        90,
                        91,
                        92,
                        93,
                        94,
                        95
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'isPigLoaded')": [
                        98,
                        99,
                        100,
                        101,
                        102,
                        103,
                        104
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'init(Configuration)')": [
                        110,
                        111,
                        112,
                        113
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'prepareForWrite(RecordConsumer)')": [
                        118,
                        119
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'write(T)')": [
                        124,
                        125,
                        126,
                        127
                    ]
                },
                "new": {
                    "(None, None)": [
                        27,
                        28,
                        30,
                        31,
                        32,
                        33,
                        34
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', None)": [
                        36,
                        46,
                        54,
                        61
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'setThriftClass(Configuration,Class)')": [
                        39
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'getThriftClass(Configuration)')": [
                        43
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'init(Configuration)')": [
                        66
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'prepareForWrite(RecordConsumer)')": [
                        71
                    ],
                    "('parquet.hadoop.thrift#ThriftWriteSupport', 'write(T)')": [
                        76
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/thrift/ThriftMetaData.java": {
                "old": {
                    "('parquet.thrift#ThriftMetaData', 'getThriftClass(String)')": [
                        74,
                        75,
                        76
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/thrift/ThriftParquetWriter.java": {
                "old": {
                    "(None, None)": [
                        29
                    ],
                    "('parquet.thrift#ThriftParquetWriter', None)": [
                        41,
                        45,
                        49
                    ]
                },
                "new": {
                    "(None, None)": [
                        29
                    ],
                    "('parquet.thrift#ThriftParquetWriter', None)": [
                        41,
                        45,
                        49
                    ]
                }
            }
        }
    },
    "80417356f04c5ee1cd6f636e9b043db3f2de24f2": {
        "authored_data": "2015 Feb 03 20:53",
        "commit.message": "PARQUET-173: Fixes `StatisticsFilter` for `And` filter predicate\n\n<!-- Reviewable:start -->\n[<img src=\"https://reviewable.io/review_button.png\" height=40 alt=\"Review on Reviewable\"/>](https://reviewable.io/reviews/apache/incubator-parquet-mr/108)\n<!-- Reviewable:end -->\n\nAuthor: Cheng Lian <lian@databricks.com>\n\nCloses #108 from liancheng/PARQUET-173 and squashes the following commits:\n\nd188f0b [Cheng Lian] Fixes test case\nbe2c8a1 [Cheng Lian] Fixes `StatisticsFilter` for `And` filter predicate\n",
        "commit.author.name": "Cheng Lian",
        "pcid": "3df3372a1ee7b6ea74af89f53a614895b8078609",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/filter2/statisticslevel/StatisticsFilter.java": {
                "old": {
                    "('parquet.filter2.statisticslevel#StatisticsFilter', 'visit(And)')": [
                        245
                    ]
                },
                "new": {
                    "('parquet.filter2.statisticslevel#StatisticsFilter', 'visit(And)')": [
                        245,
                        246,
                        247,
                        248,
                        249
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/filter2/statisticslevel/TestStatisticsFilter.java": {
                "old": {
                    "('parquet.filter2.statisticslevel#TestStatisticsFilter', 'testAnd')": [
                        225,
                        226
                    ]
                },
                "new": {
                    "('parquet.filter2.statisticslevel#TestStatisticsFilter', 'testAnd')": [
                        225,
                        226
                    ]
                }
            }
        }
    }
}