{
    "5ab09189cf32c76a3bdd8fecedaf337735c1ea24": {
        "authored_data": "2013 May 07 18:34",
        "commit.message": "update dependencies to hadoop-client\n",
        "commit.author.name": "julien",
        "pcid": "a67ea4ea283481abd324a0a799d6a09cc8002545",
        "changes": {
            "parquet-test-hadoop2/src/test/java/parquet/hadoop2/TestInputOutputFormat.java": {
                "old": {
                    "('parquet.hadoop2#TestInputOutputFormat', 'testReadWrite')": [
                        74,
                        75
                    ]
                },
                "new": {
                    "(None, None)": [
                        19
                    ],
                    "('parquet.hadoop2#TestInputOutputFormat', 'testReadWrite')": [
                        75,
                        76,
                        123
                    ]
                }
            }
        }
    },
    "a20750a87d69823e4048d954627494505d508e2e": {
        "authored_data": "2013 May 07 15:03",
        "commit.message": "Replace JobContext#getConfiguration calls with reflective call.\n",
        "commit.author.name": "Tom White",
        "pcid": "6c1ccb7e155319faa58ff7add19f87315ab5a3d1",
        "changes": {
            "parquet-avro/src/main/java/parquet/avro/AvroParquetOutputFormat.java": {
                "old": {
                    "('parquet.avro#AvroParquetOutputFormat', 'setSchema(Job,Schema)')": [
                        30
                    ]
                },
                "new": {
                    "(None, None)": [
                        23
                    ],
                    "('parquet.avro#AvroParquetOutputFormat', 'setSchema(Job,Schema)')": [
                        31
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputFormat', 'setReadSupportClass(Job,Class)')": [
                        67
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'createRecordReader(InputSplit,TaskAttemptContext)')": [
                        115
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(JobContext)')": [
                        215
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getFooters(JobContext)')": [
                        254
                    ]
                },
                "new": {
                    "(None, None)": [
                        45
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'setReadSupportClass(Job,Class)')": [
                        68
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'createRecordReader(InputSplit,TaskAttemptContext)')": [
                        116
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getSplits(JobContext)')": [
                        216
                    ],
                    "('parquet.hadoop#ParquetInputFormat', 'getFooters(JobContext)')": [
                        255
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetOutputCommitter.java": {
                "old": {
                    "('parquet.hadoop#ParquetOutputCommitter', 'commitJob(JobContext)')": [
                        45
                    ]
                },
                "new": {
                    "(None, None)": [
                        31
                    ],
                    "('parquet.hadoop#ParquetOutputCommitter', 'commitJob(JobContext)')": [
                        46
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetOutputFormat.java": {
                "old": {
                    "('parquet.hadoop#ParquetOutputFormat', 'setWriteSupportClass(Job,Class)')": [
                        86
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getWriteSupportClass(JobContext)')": [
                        90
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setBlockSize(Job,int)')": [
                        106
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setPageSize(Job,int)')": [
                        110
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setCompression(Job,CompressionCodecName)')": [
                        114
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getBlockSize(JobContext)')": [
                        118
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getPageSize(JobContext)')": [
                        122
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getCompression(JobContext)')": [
                        126
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'isCompressionSet(JobContext)')": [
                        130
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(TaskAttemptContext,Path)')": [
                        165
                    ]
                },
                "new": {
                    "(None, None)": [
                        36
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setWriteSupportClass(Job,Class)')": [
                        87
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getWriteSupportClass(JobContext)')": [
                        91
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setBlockSize(Job,int)')": [
                        107
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setPageSize(Job,int)')": [
                        111
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'setCompression(Job,CompressionCodecName)')": [
                        115
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getBlockSize(JobContext)')": [
                        119
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getPageSize(JobContext)')": [
                        123
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getCompression(JobContext)')": [
                        127
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'isCompressionSet(JobContext)')": [
                        131
                    ],
                    "('parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(TaskAttemptContext,Path)')": [
                        166
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetRecordReader.java": {
                "old": {
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,TaskAttemptContext)')": [
                        153
                    ]
                },
                "new": {
                    "(None, None)": [
                        34
                    ],
                    "('parquet.hadoop#ParquetRecordReader', 'initialize(InputSplit,TaskAttemptContext)')": [
                        154
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/example/ExampleOutputFormat.java": {
                "old": {
                    "('parquet.hadoop.example#ExampleOutputFormat', 'setSchema(Job,MessageType)')": [
                        43
                    ],
                    "('parquet.hadoop.example#ExampleOutputFormat', 'getSchema(Job)')": [
                        52
                    ]
                },
                "new": {
                    "(None, None)": [
                        23
                    ],
                    "('parquet.hadoop.example#ExampleOutputFormat', 'setSchema(Job,MessageType)')": [
                        44
                    ],
                    "('parquet.hadoop.example#ExampleOutputFormat', 'getSchema(Job)')": [
                        53
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/example/TestInputOutputFormat.java": {
                "old": {
                    "('parquet.hadoop.example#TestInputOutputFormat', None)": [
                        50
                    ]
                },
                "new": {
                    "(None, None)": [
                        42
                    ],
                    "('parquet.hadoop.example#TestInputOutputFormat', None)": [
                        51
                    ]
                }
            },
            "parquet-pig/src/main/java/parquet/pig/ParquetLoader.java": {
                "old": {
                    "('parquet.pig#ParquetLoader', 'setLocation(String,Job)')": [
                        80
                    ]
                },
                "new": {
                    "(None, None)": [
                        40
                    ],
                    "('parquet.pig#ParquetLoader', 'setLocation(String,Job)')": [
                        81
                    ]
                }
            },
            "parquet-pig/src/test/java/parquet/pig/PerfTest2.java": {
                "old": {
                    "('parquet.pig#PerfTest2', 'write(String)')": [
                        108,
                        109,
                        118
                    ],
                    "('parquet.pig#PerfTest2', 'load(String,int,StringBuilder)')": [
                        163,
                        168
                    ]
                },
                "new": {
                    "(None, None)": [
                        49
                    ],
                    "('parquet.pig#PerfTest2', 'write(String)')": [
                        109,
                        110,
                        119
                    ],
                    "('parquet.pig#PerfTest2', 'load(String,int,StringBuilder)')": [
                        164,
                        169
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/hadoop/thrift/ParquetThriftBytesOutputFormat.java": {
                "old": {
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setThriftClass(Job,Class)')": [
                        29
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'getThriftClass(Job)')": [
                        33
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setTProtocolClass(Job,Class)')": [
                        37
                    ]
                },
                "new": {
                    "(None, None)": [
                        25
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setThriftClass(Job,Class)')": [
                        30
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'getThriftClass(Job)')": [
                        34
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftBytesOutputFormat', 'setTProtocolClass(Job,Class)')": [
                        38
                    ]
                }
            },
            "parquet-thrift/src/main/java/parquet/hadoop/thrift/ParquetThriftOutputFormat.java": {
                "old": {
                    "('parquet.hadoop.thrift#ParquetThriftOutputFormat', 'setThriftClass(Job,Class)')": [
                        31
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftOutputFormat', 'getThriftClass(Job)')": [
                        35
                    ]
                },
                "new": {
                    "(None, None)": [
                        21
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftOutputFormat', 'setThriftClass(Job,Class)')": [
                        32
                    ],
                    "('parquet.hadoop.thrift#ParquetThriftOutputFormat', 'getThriftClass(Job)')": [
                        36
                    ]
                }
            },
            "parquet-thrift/src/test/java/parquet/hadoop/thrift/TestThriftToParquetFileWriter.java": {
                "old": {
                    "('parquet.hadoop.thrift#TestThriftToParquetFileWriter', 'testWriteFile')": [
                        90,
                        94
                    ]
                },
                "new": {
                    "(None, None)": [
                        52
                    ],
                    "('parquet.hadoop.thrift#TestThriftToParquetFileWriter', 'testWriteFile')": [
                        91,
                        95
                    ]
                }
            }
        }
    },
    "f5ab5ebd5e3c9e3a63f2dcc124b60fb3349679b3": {
        "authored_data": "2013 May 03 16:45",
        "commit.message": "better error message when schema is unknown\n",
        "commit.author.name": "julien",
        "pcid": "cec7b39ea182757d43e247241c2a4f64e1b04a24",
        "changes": {
            "parquet-pig/src/main/java/parquet/pig/ParquetStorer.java": {
                "old": {
                    "('parquet.pig#ParquetStorer', 'getSchema')": [
                        68
                    ]
                },
                "new": {
                    "('parquet.pig#ParquetStorer', 'getSchema')": [
                        68,
                        69,
                        70,
                        71,
                        72
                    ]
                }
            }
        }
    },
    "75ead0a3ed59c4b821fc2973b8917a856d3067dc": {
        "authored_data": "2013 May 01 23:08",
        "commit.message": "turn LOGs back to INFO\n",
        "commit.author.name": "julien",
        "pcid": "a1fbcfb1fd229f77222fabae54a97f3b161f4d26",
        "changes": {
            "parquet-column/src/main/java/parquet/Log.java": {
                "old": {
                    "('parquet#Log', None)": [
                        46
                    ]
                },
                "new": {
                    "('parquet#Log', None)": [
                        46
                    ]
                }
            }
        }
    },
    "a1fbcfb1fd229f77222fabae54a97f3b161f4d26": {
        "authored_data": "2013 May 01 23:05",
        "commit.message": "make total size include header size\n",
        "commit.author.name": "julien",
        "pcid": "2d4be4391e17612fbfec132adf6b5b1dc1b40802",
        "changes": {
            "parquet-column/src/main/java/parquet/Log.java": {
                "old": {
                    "('parquet#Log', None)": [
                        46
                    ]
                },
                "new": {
                    "('parquet#Log', None)": [
                        46
                    ]
                }
            },
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetFileWriter.java": {
                "old": {
                    "('parquet.hadoop#ParquetFileWriter', 'writeDataPage(int,int,BytesInput,parquet,parquet,parquet)')": [
                        191,
                        200,
                        201
                    ],
                    "('parquet.hadoop#ParquetFileWriter', 'writeDataPages(BytesInput,long,long,List)')": [
                        219,
                        220
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetFileWriter', 'writeDataPage(int,int,BytesInput,parquet,parquet,parquet)')": [
                        191,
                        192,
                        201,
                        202,
                        203
                    ],
                    "('parquet.hadoop#ParquetFileWriter', 'writeDataPages(BytesInput,long,long,List)')": [
                        221,
                        222,
                        223
                    ],
                    "('parquet.hadoop#ParquetFileWriter', None)": [
                        313,
                        314,
                        315,
                        316,
                        317,
                        318
                    ],
                    "('parquet.hadoop#ParquetFileWriter', 'getPos')": [
                        319,
                        320
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/TestParquetFileWriter.java": {
                "new": {
                    "('parquet.hadoop#TestParquetFileWriter', 'testWriteRead')": [
                        77,
                        81,
                        83,
                        88,
                        102,
                        103,
                        104
                    ]
                }
            }
        }
    }
}