{
    "f9a867689a18e33cb95fbd21b10fcd5b648739be": {
        "authored_data": "2014 Apr 03 05:10",
        "commit.message": "stop using strings and b64 for compressed input splits\n",
        "commit.author.name": "Dmitriy Ryaboy",
        "pcid": "c54cad5e4a54dbbae417bc1561c623b1267f2079",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputSplit.java": {
                "old": {
                    "(None, None)": [
                        36,
                        38
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readFields(DataInput)')": [
                        152,
                        153
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'write(DataOutput)')": [
                        168,
                        169
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'compressString(String)')": [
                        174,
                        186,
                        187
                    ],
                    "('parquet.hadoop#ParquetInputSplit', None)": [
                        188,
                        189
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'decompressString(String)')": [
                        190,
                        191,
                        192,
                        207,
                        214
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readKeyValues(DataInput)')": [
                        292,
                        293
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeKeyValues(DataOutput,Map)')": [
                        305,
                        306
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputSplit', 'readFields(DataInput)')": [
                        150,
                        151
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'write(DataOutput)')": [
                        166,
                        167,
                        168,
                        169,
                        170,
                        171
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'compressString(String)')": [
                        176,
                        188
                    ],
                    "('parquet.hadoop#ParquetInputSplit', None)": [
                        189,
                        190,
                        196,
                        197
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'decompressString(DataInput)')": [
                        191,
                        192,
                        193,
                        194,
                        195
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'decompressString(byte)')": [
                        198,
                        199,
                        214,
                        221
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'readKeyValues(DataInput)')": [
                        299,
                        300
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeKeyValues(DataOutput,Map)')": [
                        312,
                        313,
                        314,
                        315,
                        316,
                        317
                    ]
                }
            },
            "parquet-hadoop/src/test/java/parquet/hadoop/TestParquetInputSplit.java": {
                "old": {
                    "('parquet.hadoop#TestParquetInputSplit', 'testStringCompression')": [
                        40,
                        41
                    ]
                },
                "new": {
                    "('parquet.hadoop#TestParquetInputSplit', 'testStringCompression')": [
                        40,
                        41
                    ]
                }
            }
        }
    },
    "253eb6a182b1abe746aa792eae9ddf9389d99b61": {
        "authored_data": "2014 Apr 02 18:04",
        "commit.message": "select * from parquet hive table containing map columns runs into exception. Issue #341.\n",
        "commit.author.name": "Szehon Ho",
        "pcid": "5207422b349f30ec26958be86dc7390b46d63990",
        "changes": {
            "parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java": {
                "old": {
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', 'equals(Object)')": [
                        144,
                        145,
                        146,
                        149,
                        150
                    ],
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', None)": [
                        155,
                        156
                    ],
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', 'hashCode')": [
                        157,
                        158,
                        159,
                        160,
                        161
                    ],
                    "(None, None)": [
                        162
                    ]
                },
                "new": {
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', 'hashCode')": [
                        137,
                        138,
                        139,
                        140,
                        141,
                        142,
                        143,
                        144
                    ],
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', None)": [
                        145,
                        146,
                        147
                    ],
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#AbstractParquetMapInspector', 'equals(Object)')": [
                        149,
                        150,
                        151,
                        158,
                        159,
                        160,
                        161,
                        162,
                        163,
                        166,
                        167,
                        168,
                        169,
                        170
                    ]
                }
            }
        }
    },
    "c54cad5e4a54dbbae417bc1561c623b1267f2079": {
        "authored_data": "2014 Apr 02 01:51",
        "commit.message": "compress kv pairs in ParquetInputSplits\n",
        "commit.author.name": "Dmitriy Ryaboy",
        "pcid": "616f7783fb7fc527c4bbf559071e2b068ea916a9",
        "changes": {
            "parquet-hadoop/src/main/java/parquet/hadoop/ParquetInputSplit.java": {
                "old": {
                    "('parquet.hadoop#ParquetInputSplit', 'readKeyValues(DataInput)')": [
                        292,
                        293
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeKeyValues(DataOutput,Map)')": [
                        305,
                        306
                    ]
                },
                "new": {
                    "('parquet.hadoop#ParquetInputSplit', 'readKeyValues(DataInput)')": [
                        292,
                        293
                    ],
                    "('parquet.hadoop#ParquetInputSplit', 'writeKeyValues(DataOutput,Map)')": [
                        305,
                        306
                    ]
                }
            }
        }
    },
    "156b186bac66598c1eb6f81c2507fe4f116575d8": {
        "authored_data": "2014 Apr 02 01:28",
        "commit.message": "remove duplicate code\n",
        "commit.author.name": "tongjiechen",
        "pcid": "47ff4ab39a22094968b2868b2f0138c5ccbeec08",
        "changes": {
            "parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java": {
                "old": {
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#ArrayWritableObjectInspector', 'getStructFieldData(Object,StructField)')": [
                        129,
                        130,
                        131,
                        132,
                        133,
                        134,
                        135
                    ]
                }
            }
        }
    },
    "82ec5842372645535e1602df8c51ec05d683d5bd": {
        "authored_data": "2014 Apr 02 01:00",
        "commit.message": "issue #324 remove additional tab\n",
        "commit.author.name": "tongjiechen",
        "pcid": "07c54722a2ebf6edb45a1765c8fc6a5d7bc02795",
        "changes": {
            "parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java": {
                "old": {
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#ArrayWritableObjectInspector', 'setStructFieldData(Object,StructField,Object)')": [
                        178
                    ]
                },
                "new": {
                    "('org.apache.hadoop.hive.ql.io.parquet.serde#ArrayWritableObjectInspector', 'setStructFieldData(Object,StructField,Object)')": [
                        178
                    ]
                }
            }
        }
    }
}