{
    "07a42d3ffd034e467e49b5c449d4f5f81c471cc5": {
        "authored_data": "2016 Oct 05 20:20",
        "commit.message": "PARQUET-726: Increase max difference of testMemoryManagerUpperLimit to 10%\n\nAuthor: Niels Basjes <nbasjes@bol.com>\n\nCloses #370 from nielsbasjes/PARQUET-726 and squashes the following commits:\n\nf385ede [Niels Basjes] PARQUET-726: Increase max difference of testMemoryManagerUpperLimit to 10%\n",
        "commit.author.name": "Niels Basjes",
        "pcid": "b59be86597cfcd805c24fa406af46071400e24c8",
        "changes": {
            "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestMemoryManager.java": {
                "old": {
                    "('org.apache.parquet.hadoop#TestMemoryManager', 'testMemoryManagerUpperLimit')": [
                        71,
                        73,
                        74
                    ]
                },
                "new": {
                    "('org.apache.parquet.hadoop#TestMemoryManager', 'testMemoryManagerUpperLimit')": [
                        71,
                        73,
                        74,
                        75
                    ]
                }
            }
        }
    },
    "b59be86597cfcd805c24fa406af46071400e24c8": {
        "authored_data": "2016 Oct 03 22:04",
        "commit.message": "PARQUET-674: Add InputFile abstraction for openable files.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #368 from rdblue/PARQUET-674-add-data-source and squashes the following commits:\n\n8c689e9 [Ryan Blue] PARQUET-674: Implement review comments.\n4a7c327 [Ryan Blue] PARQUET-674: Add DataSource abstraction for openable files.\n",
        "commit.author.name": "Ryan Blue",
        "pcid": "e54ca615f213f5db6d34d9163c97eec98920d7a7",
        "changes": {
            "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java": {
                "old": {
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,Path,MetadataFilter)')": [
                        413,
                        414
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,FileStatus,MetadataFilter)')": [
                        434,
                        435,
                        436,
                        437,
                        438,
                        439
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(long,String,SeekableInputStream,MetadataFilter)')": [
                        452
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', None)": [
                        566
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'getFooter')": [
                        605
                    ]
                },
                "new": {
                    "(None, None)": [
                        91,
                        97
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,Path,MetadataFilter)')": [
                        415
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,FileStatus,MetadataFilter)')": [
                        435
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', None)": [
                        436,
                        437,
                        438,
                        439,
                        440,
                        441,
                        442,
                        443,
                        444,
                        576
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(InputFile,MetadataFilter)')": [
                        445,
                        446,
                        447,
                        448,
                        449
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(ParquetMetadataConverter,long,String,SeekableInputStream,MetadataFilter)')": [
                        462
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'getFooter')": [
                        615
                    ]
                }
            }
        }
    },
    "e54ca615f213f5db6d34d9163c97eec98920d7a7": {
        "authored_data": "2016 Sep 08 21:48",
        "commit.message": "PARQUET-660: Ignore extension fields in protobuf messages.\n\nCurrently, converting protobuf messages with extension can result in an uninformative error or a data corruption. A more detailed explanation in the corresponding [jira](https://issues.apache.org/jira/browse/PARQUET-660).\n\nThis patch simply ignores extension fields in protobuf messages.\n\nIn the longer run, I'd like to add a proper support for Protobuf extensions. This might take a little longer though, so I've decided to improve the current situation with this patch.\n\nAuthor: Jakub Kukul <jakub.kukul@gmail.com>\n\nCloses #351 from jkukul/master and squashes the following commits:\n\n27580ab [Jakub Kukul] PARQUET-660: Throw Unsupported exception for messages with extensions.\ndb6e08b [Jakub Kukul] PARQUET-660: Refactor: Don't use additional variable for indexing fieldWriters.\ne910a8a [Jakub Kukul] PARQUET-660: Refactor: Add missing indentation.\n",
        "commit.author.name": "Jakub Kukul",
        "pcid": "044de16c14076019f87763b7b58c45664ee57c11",
        "changes": {
            "parquet-protobuf/src/main/java/org/apache/parquet/proto/ProtoSchemaConverter.java": {
                "old": {
                    "('org.apache.parquet.proto#ProtoSchemaConverter', 'addField(Descriptors,GroupBuilder)')": [
                        89,
                        90,
                        91,
                        92,
                        93,
                        94,
                        95,
                        96,
                        97,
                        98,
                        99,
                        100,
                        101,
                        102,
                        103
                    ]
                },
                "new": {
                    "('org.apache.parquet.proto#ProtoSchemaConverter', 'addField(Descriptors,GroupBuilder)')": [
                        89,
                        90,
                        91,
                        92,
                        93,
                        94,
                        95,
                        96,
                        97,
                        98,
                        99,
                        100,
                        101,
                        102,
                        103
                    ]
                }
            },
            "parquet-protobuf/src/main/java/org/apache/parquet/proto/ProtoWriteSupport.java": {
                "old": {
                    "('org.apache.parquet.proto#ProtoWriteSupport', None)": [
                        159,
                        172,
                        173,
                        279
                    ]
                },
                "new": {
                    "('org.apache.parquet.proto#ProtoWriteSupport', None)": [
                        171,
                        221,
                        222,
                        223,
                        224,
                        225,
                        226,
                        227,
                        284
                    ]
                }
            },
            "parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoWriteSupportTest.java": {
                "new": {
                    "('org.apache.parquet.proto#ProtoWriteSupportTest', None)": [
                        168,
                        169
                    ],
                    "('org.apache.parquet.proto#ProtoWriteSupportTest', 'testMessageWithExtensions')": [
                        170,
                        171,
                        172,
                        173,
                        174,
                        175,
                        176,
                        177,
                        178,
                        179,
                        180
                    ],
                    "(None, None)": [
                        181,
                        182
                    ]
                }
            }
        }
    },
    "044de16c14076019f87763b7b58c45664ee57c11": {
        "authored_data": "2016 Sep 08 21:22",
        "commit.message": "PARQUET-623: Fix DeltaByteArrayReader#skip.\n\nPreviously, this passed the skip to the underlying readers, but would\nnot update previous and would corrupt values or cause exceptions.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #366 from rdblue/PARQUET-623-fix-delta-byte-array-skip and squashes the following commits:\n\nf85800c [Ryan Blue] PARQUET-623: Fix DeltaByteArrayReader#skip.\n",
        "commit.author.name": "Ryan Blue",
        "pcid": "6dad1e3bd0e277f5b5e5e2a0720f474271c1648d",
        "changes": {
            "parquet-column/src/main/java/org/apache/parquet/column/values/deltastrings/DeltaByteArrayReader.java": {
                "old": {
                    "('org.apache.parquet.column.values.deltastrings#DeltaByteArrayReader', 'skip')": [
                        58,
                        59
                    ]
                },
                "new": {
                    "('org.apache.parquet.column.values.deltastrings#DeltaByteArrayReader', 'skip')": [
                        58,
                        59
                    ]
                }
            },
            "parquet-column/src/test/java/org/apache/parquet/column/values/deltastrings/TestDeltaByteArray.java": {
                "new": {
                    "(None, None)": [
                        22
                    ],
                    "('org.apache.parquet.column.values.deltastrings#TestDeltaByteArray', None)": [
                        51,
                        52,
                        53,
                        99,
                        100,
                        101
                    ],
                    "('org.apache.parquet.column.values.deltastrings#TestDeltaByteArray', 'testRandomStringsWithSkip')": [
                        54,
                        55,
                        56,
                        57
                    ],
                    "('org.apache.parquet.column.values.deltastrings#TestDeltaByteArray', 'assertReadWriteWithSkip(DeltaByteArrayWriter,DeltaByteArrayReader,String)')": [
                        92,
                        93,
                        94,
                        95,
                        96,
                        97,
                        98
                    ]
                }
            }
        }
    },
    "255f10834a67cf13518316de0e2c8a345677ebbf": {
        "authored_data": "2016 Aug 16 17:40",
        "commit.message": "PARQUET-460: merge multi parquet files to one file\n\nA merge command for parquet-tools based on https://issues.apache.org/jira/browse/PARQUET-382.\n\nAuthor: flykobe <flykobecy@gmail.com>\n\nCloses #327 from flykobe/merge_tool and squashes the following commits:\n\nb031c18 [flykobe] check input files\nda28832 [flykobe] merge multi parquet files to one file\n",
        "commit.author.name": "flykobe",
        "pcid": "898f3d0f652f313473c67fef32e22d94d8294d4f",
        "changes": {
            "parquet-tools/src/main/java/org/apache/parquet/tools/command/Registry.java": {
                "new": {
                    "('org.apache.parquet.tools.command#Registry', None)": [
                        34
                    ]
                }
            }
        }
    },
    "898f3d0f652f313473c67fef32e22d94d8294d4f": {
        "authored_data": "2016 Aug 16 17:12",
        "commit.message": "PARQUET-400: Replace CompatibilityUtil with SeekableInputStream.\n\nThis fixes PARQUET-400 by replacing `CompatibilityUtil` with `SeekableInputStream` that's implemented for hadoop-1 and hadoop-2. The benefit of this approach is that `SeekableInputStream` can be used for non-Hadoop file systems in the future.\n\nThis also changes the default Hadoop version to Hadoop-2. The library is still compatible with Hadoop 1.x, but this makes building Hadoop-2 classes, like `H2SeekableInputStream`, much easier and removes the need for multiple hadoop versions during compilation.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #349 from rdblue/PARQUET-400-byte-buffers and squashes the following commits:\n\n1bcb8a8 [Ryan Blue] PARQUET-400: Fix review nits.\n823ca00 [Ryan Blue] PARQUET-400: Add tests for Hadoop 2 readFully.\n02d3709 [Ryan Blue] PARQUET-400: Remove unused property.\nb543013 [Ryan Blue] PARQUET-400: Fix logger for HadoopStreams.\n2cb6934 [Ryan Blue] PARQUET-400: Remove H2SeekableInputStream tests.\nabaa695 [Ryan Blue] PARQUET-400: Fix review items.\n5dc50a5 [Ryan Blue] PARQUET-400: Add tests for H1SeekableInputStream methods.\n730a9e2 [Ryan Blue] PARQUET-400: Move SeekableInputStream to io package.\n506a556 [Ryan Blue] PARQUET-400: Remove Hadoop dependencies from SeekableInputStream.\nc80580c [Ryan Blue] PARQUET-400: Handle UnsupportedOperationException from read(ByteBuffer).\nba08b3f [Ryan Blue] PARQUET-400: Replace CompatibilityUtil with SeekableInputStream.\n",
        "commit.author.name": "Ryan Blue",
        "pcid": "c8d78b21b3dde3bfb36fed7cb33bd4ec3f01b8da",
        "changes": {
            "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java": {
                "old": {
                    "(None, None)": [
                        57,
                        69
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,FileStatus,MetadataFilter)')": [
                        435
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(long,String,FSDataInputStream,MetadataFilter)')": [
                        452
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', None)": [
                        496,
                        534,
                        565,
                        588,
                        943,
                        951,
                        967,
                        1053,
                        1057
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readCompressedDictionary(PageHeader,FSDataInputStream)')": [
                        775
                    ]
                },
                "new": {
                    "(None, None)": [
                        92,
                        93
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(Configuration,FileStatus,MetadataFilter)')": [
                        435
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readFooter(long,String,SeekableInputStream,MetadataFilter)')": [
                        452
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', None)": [
                        496,
                        534,
                        565,
                        588,
                        943,
                        951,
                        967,
                        1053,
                        1056,
                        1057,
                        1059,
                        1060
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileReader', 'readCompressedDictionary(PageHeader,SeekableInputStream)')": [
                        775
                    ]
                }
            },
            "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java": {
                "old": {
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'copy(FSDataInputStream,FSDataOutputStream,long,long)')": [
                        599,
                        600
                    ]
                },
                "new": {
                    "(None, None)": [
                        64,
                        65
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'appendRowGroups(FSDataInputStream,List,boolean)')": [
                        500
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', None)": [
                        501,
                        502,
                        514,
                        515
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'appendRowGroups(SeekableInputStream,List,boolean)')": [
                        503,
                        504,
                        505
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'appendRowGroup(FSDataInputStream,BlockMetaData,boolean)')": [
                        513
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'appendRowGroup(SeekableInputStream,BlockMetaData,boolean)')": [
                        516,
                        517
                    ],
                    "('org.apache.parquet.hadoop#ParquetFileWriter', 'copy(SeekableInputStream,FSDataOutputStream,long,long)')": [
                        612,
                        613
                    ]
                }
            }
        }
    },
    "30aa91012cf6019bb9720609c1d03b5386a87ffb": {
        "authored_data": "2016 Aug 11 20:30",
        "commit.message": "PARQUET-601: Add support to configure the encoding used by ValueWriters\n\n### Context:\nParquet is currently structured to choose the appropriate value writer based on the type of the column as well as the Parquet version. As of now, the writer(s) (and hence encoding) for each data type is hard coded in the Parquet source code.\n\nThis PR adds support for being able to override the encodings per type via config. That allows users to experiment with various encoding strategies manually as well as enables them to override the hardcoded defaults if they don't suit their use case.\n\nWe can override encodings per data type (int32 / int64 / ...).\nSomething on the lines of:\n```\nparquet.writer.encoding-override.<type> = \"encoding1[,encoding2]\"\n```\n\nAs an example:\n```\n\"parquet.writer.encoding-override.int32\" = \"plain\"\n(Chooses Plain encoding and hence the PlainValuesWriter).\n```\n\nWhen a primary + fallback need to be specified, we can do the following:\n```\n\"parquet.writer.encoding-override.binary\" = \"rle_dictionary,delta_byte_array\"\n(Chooses RLE_DICTIONARY encoding as the initial encoding and DELTA_BYTE_ARRAY encoding as the fallback and hence creates a FallbackWriter(PlainBinaryDictionaryValuesWriter, DeltaByteArrayWriter).\n```\n\nIn such cases we can mandate that the first encoding listed must allow for Fallbacks by implementing [RequiresFallback](https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/values/RequiresFallback.java#L31).\n\n### PR notes:\n\n- Restructured the ValuesWriter creation code. Pulled it out of ParquetProperties into a new class and refactored the flow based on type as it was getting hard to follow and I felt adding the overrides would make it harder. Added a bunch of unit tests to verify the ValuesWriter we create for combinations of type, parquet version and dictionary on / off.\n- Added unit tests to verify parsing of the encoding overrides + creation of ValuesWriters based on these overrides.\n- Manually tested some encoding overrides scenarios out on Hadoop (both parquet v1, v2).\n\nAuthor: Piyush Narang <pnarang@twitter.com>\n\nCloses #342 from piyushnarang/master and squashes the following commits:\n\n3ebab28 [Piyush Narang] Remove Configurable\n149bb98 [Piyush Narang] Switch to getValuesWriterFactory call to non-static\n0b78e04 [Piyush Narang] Address Ryan's feedback\n1da6ca3 [Piyush Narang] Merge branch 'master' into piyush/dynamic-encoding-overrides\nf021ed2 [Piyush Narang] Tweak comment in ValuesWriterFactory\ncb02ea0 [Piyush Narang] Fix review comments\nbf4bc6d [Piyush Narang] Add support for Config setting in ValuesWriter factory\n8a852a3 [Piyush Narang] Log values writer factory chosen\ne4b61a4 [Piyush Narang] Tweak factory instantiation a bit\nb46cccd [Piyush Narang] Add class based factory override\n6a5428f [Piyush Narang] Clean up some stuff in ValuesWriterFactory\n0f8cd09 [Piyush Narang] Refactor mockito version\n9ead61d [Piyush Narang] Add guava test dep\n5c636c7 [Piyush Narang] Add encoding-overrides config to ParquetOutputFormat config\nb9d6c13 [Piyush Narang] Refactor code in ValuesWriterFactory a bit\nff4c90d [Piyush Narang] Pull out value writer creation to ValuesWriterFactory and add unit tests\n",
        "commit.author.name": "Piyush Narang",
        "pcid": "b301d12700acf5313de33785857f88f60bcb053a",
        "changes": {
            "parquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java": {
                "old": {
                    "(None, None)": [
                        27,
                        28,
                        29,
                        35,
                        36,
                        37,
                        38,
                        39,
                        40,
                        41,
                        42,
                        43,
                        44,
                        45,
                        46,
                        47,
                        48,
                        418
                    ],
                    "('org.apache.parquet.column#ParquetProperties', None)": [
                        100,
                        101,
                        104,
                        107,
                        163,
                        164,
                        165,
                        166,
                        201,
                        202,
                        203,
                        228,
                        229,
                        230,
                        239,
                        240,
                        241,
                        262,
                        415,
                        417
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'plainWriter(ColumnDescriptor)')": [
                        147,
                        148,
                        149,
                        150,
                        151,
                        152,
                        153,
                        154,
                        155,
                        156,
                        157,
                        158,
                        159,
                        160,
                        161,
                        162
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'dictionaryWriter(ColumnDescriptor)')": [
                        167,
                        168,
                        169,
                        170,
                        171,
                        172,
                        173,
                        174,
                        175,
                        176,
                        177,
                        178,
                        179,
                        180,
                        181,
                        182,
                        183,
                        184,
                        185,
                        186,
                        187,
                        188,
                        189,
                        190,
                        191,
                        192,
                        193,
                        194,
                        195,
                        196,
                        197,
                        198,
                        199,
                        200
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'writerToFallbackTo(ColumnDescriptor)')": [
                        204,
                        205,
                        206,
                        207,
                        208,
                        209,
                        210,
                        211,
                        212,
                        213,
                        214,
                        215,
                        216,
                        217,
                        218,
                        219,
                        220,
                        221,
                        222,
                        223,
                        224,
                        225,
                        226,
                        227
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'dictWriterWithFallBack(ColumnDescriptor)')": [
                        231,
                        232,
                        233,
                        234,
                        235,
                        236,
                        237,
                        238
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'newValuesWriter(ColumnDescriptor)')": [
                        243,
                        244,
                        245,
                        246,
                        247,
                        248,
                        249,
                        250,
                        251,
                        252,
                        253,
                        254,
                        255,
                        256,
                        257,
                        258,
                        259,
                        260,
                        261
                    ]
                },
                "new": {
                    "(None, None)": [
                        32,
                        35,
                        317,
                        318
                    ],
                    "('org.apache.parquet.column#ParquetProperties', None)": [
                        53,
                        54,
                        79,
                        88,
                        91,
                        92,
                        95,
                        103,
                        104,
                        143,
                        144,
                        187,
                        188,
                        210,
                        299,
                        300,
                        301,
                        302,
                        303,
                        304,
                        306,
                        307,
                        309,
                        310,
                        311,
                        312,
                        313,
                        314,
                        315,
                        316
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'newValuesWriter(ColumnDescriptor)')": [
                        138
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'getInitialSlabSize')": [
                        145,
                        146
                    ],
                    "('org.apache.parquet.column#ParquetProperties', 'getValuesWriterFactory')": [
                        185,
                        186
                    ]
                }
            },
            "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java": {
                "old": {
                    "('org.apache.parquet.hadoop#ParquetOutputFormat', None)": [
                        320
                    ],
                    "('org.apache.parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(Configuration,Path,CompressionCodecName)')": [
                        377,
                        378,
                        379,
                        380,
                        381,
                        382,
                        383,
                        384,
                        385,
                        386
                    ]
                },
                "new": {
                    "('org.apache.parquet.hadoop#ParquetOutputFormat', 'getRecordWriter(Configuration,Path,CompressionCodecName)')": [
                        376,
                        377,
                        378,
                        379,
                        380,
                        381,
                        382,
                        383,
                        384,
                        385,
                        386,
                        387
                    ]
                }
            }
        }
    }
}